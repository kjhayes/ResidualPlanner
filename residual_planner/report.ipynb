{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "441e1363-1fbf-45ad-9c56-7e22d0d3a7b7",
   "metadata": {},
   "source": [
    "# CS 396 - Differential Privacy Final Project - Kevin Hayes\n",
    "\n",
    "For this project, my original intent was to try and modify the ResidualPlanner (https://github.com/dkifer/ResidualPlanner) codebase so that it would support using the infinity-norm instead of the l2-norm. Sadly, I have failed in that regard, it's been a few weeks of first understanding the code and associated paper well enough to start modifying it, and I think I've come to understand the paper itself pretty well, but actually changing the underlying mechanism is decidedly beyond my abilities.\n",
    "\n",
    "So instead of making the modification myself, I've decided to shift my focus to helping summarize the paper and code-base (1) because I think both are very interesting now that I'm understanding them more, and would like to share that with you and (2) so that if someone better equipped, Professor Dong, a graduate student, or a future undergraduate in CS 396 wants to try tackling the problem of getting ResidualPlanner to use the infinity-norm, I hope this document will give them a jumping off point, so that they don't need to start from scratch. With that goal in mind, I'll try to explain some topics/background that I'm sure Professor Dong already knows, but I want this to be a more generally approachable document.\n",
    "\n",
    "I'm also formatting this as a jupyter notebook so that I can provide interactive examples of residual planner itself to help demonstrate the different parts of the code base. I'm also aiming for a more casual/intuition focused approach to the summary (if you want rigor, you should probably just read the paper itself).\n",
    "\n",
    "(This document is meant to be run within the residual_planner repository itself, so that the code blocks will actually work correctly)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb0ae81-9c10-4674-bb82-87240bb7845c",
   "metadata": {},
   "source": [
    "### Kronecker Product\n",
    "First, this paper depends heavily on the \"Kronecker Product\" $\\otimes$ which is defined on two matrices, and for every element of the left matrix, it does a scalar/matrix multiplication with the right matrix, and expands the block matrix in place. For example:\n",
    "$$\n",
    "\\begin{bmatrix}a & b \\\\ c & d\\end{bmatrix} \\otimes \\begin{bmatrix}e & f \\\\ g & h\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    "  a\\begin{bmatrix}e & f \\\\ g & h\\end{bmatrix} & \n",
    "  b\\begin{bmatrix}e & f \\\\ g & h\\end{bmatrix} \\\\ \n",
    "  c\\begin{bmatrix}e & f \\\\ g & h\\end{bmatrix} & \n",
    "  d\\begin{bmatrix}e & f \\\\ g & h\\end{bmatrix}\n",
    "\\end{bmatrix}\n",
    "=\\begin{bmatrix}\n",
    " a \\cdot e & a \\cdot f & b \\cdot e & b \\cdot f\\\\\n",
    " a \\cdot g & a \\cdot h & b \\cdot g & b \\cdot h\\\\\n",
    " c \\cdot e & c \\cdot f & d \\cdot e & d \\cdot f\\\\\n",
    " c \\cdot g & c \\cdot h & d \\cdot g & d \\cdot h\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "When thinking about the Kronecker product in this paper, I think it inuitively makes the most sense to think of it as a form of cartesian product, taking every possible pairing of two elements taken from each matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f65ab81-1d7c-42d3-904b-112024a9a03b",
   "metadata": {},
   "source": [
    "\n",
    "Two important properties of kronecker products which we will use are the \"mixed-product\" property, which tells us for matrices $A$, $B$, $C$, and $D$:\n",
    "$$\n",
    "(A \\otimes B)(C \\otimes D) = AC \\otimes BD\n",
    "$$\n",
    "\n",
    "And that the kronecker product is bilinear, particularly the fact that\n",
    "$$\n",
    "A \\otimes (B + C) = (A \\otimes B) + (A \\otimes C)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51979cc3-bac4-4e62-ae2b-2a68d37e8485",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "ResidualPlanner often refers to \"attributes\" as the abstract data which is being stored within the database. Put simply these are best thought of as \"enum\" from programming languages like C, C++, Java, etc. It is a range of N values which represent something, but ultimately can be represented as a single number.\n",
    "This is important because it means that the domain for every one of our attributes is precisely defined, and we don't need to worry about the actual meanings behind any attribute.\n",
    "\n",
    "Something important to note though is that encoding attributes this way does pretty severely limit the types of data which we can store within a data base and still reasonably apply ResidualPlanner to. If we want to allow a \"string\" field that can be up to 32 ascii characters, then the \"attribute\" that represents this field would have a domain of size $256^{32} = 2^{256}$ which is too large to ever be reasonably computed with the current system.\n",
    "\n",
    "Representing all attributes this way is important because if we know the domain of every attribute, then we can represent our database with a \"one-hot\" encoding. Conceptually, a one-hot encoding is a row vector of length N, where N is the number of possible unique tuples in our database, and we assign each index to a specific tuple. In some ways this is a \"brute-force\" approach to representing our data, and it seems like this shouldn't ever work, combinatorics are working against us (even if our only attribute is a single 32-character string as mentioned before, then $N=2^{256}$). It's actually useful for two reasons though, first, our vectors here are almost always extremely sparse, so we can encode long strings of zeros much more efficiently, and second, because when concerning privacy, fields like arbitrary strings are rare. You want the data to be anonymous in the first place, but if we actually have a large number of unique strings in our database, then they can be used as a UUID, and we can't exactly add noise to a string.\n",
    "\n",
    "Then we can represent a single tuple in our database as an N length vector with a single 1, in the index corresponding to the tuples specific value. And then then the entire database can be represented as the sum of all of these vectors, meaning that for duplicates in our database we just keep the number of duplicates.\n",
    "\n",
    "One important thing to note, is that we can use kronecker products to generate these \"one-hot\"encodings in the first place. If we take each attribute, and encode it's values as the unit vectors with a single 1 and all other elements 0, of some $|Attr|$ sized vector space, then a specific row can be encoded as the kronecker product\n",
    "$$\n",
    "\\text{one-hot-encode-row}(row) = \\bigotimes_{attr \\in D} \\text{one-hot-encode-attribute}(row_{attr})\n",
    "$$\n",
    "which will result in a one-hot encoded version of our row.\n",
    "\n",
    "We will need to impose some order on the attributes as well, because the kronecker product is not associative, and for all kronecker products of the form $\\bigotimes_{attr \\in D}$, we will use the same order."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de6ff33-1b31-4278-928c-7fe4edff2e1c",
   "metadata": {},
   "source": [
    "### Workloads\n",
    "\n",
    "The paper represents all queries on our database as \"counts\" of some sort. As we saw in class, this isn't too much of a problem, and many more complicated types of queries can be represented as counts, because we could issue the \"count\" of a specific row(s) in the database to make arbitrary queries.\n",
    "\n",
    "Specifically a query is a set of attribute(s) or the null-set\n",
    "$$\n",
    "\\emptyset,\\\\\n",
    "\\{Attr_1\\},\\\\\n",
    "\\{Attr_1, Attr_2\\}\n",
    "$$\n",
    "Where the null-set indicates the count of the entire database, and a query with X attributes would compute the cartesian product of every attributes domain in the set, and then for every element in the cartesian set, return the count of rows in the database which have those values. In this way, then a query containing every attribute in our database, should return the \"sum of one-hot encodings\" database itself."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9252a8-c651-451e-81cb-bea9ba2b5494",
   "metadata": {},
   "source": [
    "### Questions Answered by Each Query\n",
    "\n",
    "So far all we have seen is terminology, but we have enough now to look at one of the first really interesting thoughts which the paper has, that we can extract out the minimal amount of extra information each query has from it's subqueries.\n",
    "\n",
    "To help explain, consider a database with contains the attribute \"IsAlive\" whose domain is $\\{True,False\\}$, then consider the query\n",
    "$$\n",
    "\\{IsAlive\\}\n",
    "$$\n",
    "\n",
    "This should give us two numbers in response, the number of rows in our database with \"True\" (how many people are alive?) and the number of rows with \"False\" (how many people are dead?). In addition to those questions, we can notice that we can also derive the query to the question: \"How many people are there total?\" but adding up both numbers. This query is represented by the null-set $\\emptyset$.\n",
    "\n",
    "Importantly from this, the paper notes that from the answer to any query $Q$, we can derive the answer to the query represented by any subset of $Q$.\n",
    "So if we answer the query\n",
    "$$\n",
    "\\{Attr_1,Attr_2\\}\n",
    "$$\n",
    "\n",
    "We've also implicitly answered the queries:\n",
    "$$\n",
    "\\{Attr_1\\}, \\{Attr_2\\}, \\emptyset\n",
    "$$\n",
    "\n",
    "This is extremely important for keeping consistent answers, because if both $\\{IsAlive\\}$ and $\\emptyset$ are submitted as queries, then we need to make sure that after adding noise that they both agree on the total number of rows in the dataset.\n",
    "\n",
    "Really, the only additional information provided by $\\{IsAlive\\}$ if we already know $\\emptyset$ is the relative values of each value of the attribute. Trying to model that additional information, is what motivates creating the idea of \"subtraction\" matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628a42f0-1deb-4d79-8a63-2eec6da9fa46",
   "metadata": {},
   "source": [
    "### Subtraction Matrices\n",
    "\n",
    "ResidualPlanner defines the new term \"subtraction\" matrices, of the form $S_i$ where the $i$-th has $i-1$ rows and $i$ columns, where the leftmost column is all ones, and the rest of the rows form $-I_{i \\times i}$ so for example:\n",
    "\n",
    "$$\n",
    "S_2 = \\begin{bmatrix} 1 & -1 \\end{bmatrix}\\\\\n",
    "$$\n",
    "$$\n",
    "S_3 = \\begin{bmatrix} 1 & -1 & 0 \\\\ 1 & 0 & -1 \\end{bmatrix}\n",
    "$$\n",
    "$$\n",
    "S_4 =\n",
    "\\begin{bmatrix}\n",
    "1 & -1 & 0 & 0\\\\\n",
    "1 & 0 & -1 & 0\\\\\n",
    "1 & 0 & 0 & -1\\\\\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "And we define $S_0$ to be $\\begin{bmatrix}1\\end{bmatrix}$.\n",
    "\n",
    "The paper introduces and uses subtraction matrices, before they've actually given any form of motivation behind why they take the form they do, and what they are actually useful for. Which meant it took me far too long to wrap my head around them.\n",
    "\n",
    "If we multiply the $i$ long vector of answers to a query by the corresponding $S_i$ subtraction matrix, then we only end up with the differences between the first entry and all the rest. Effectively \"losing\" the information about the absolute count, and only leaving behind the relative values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ccf4eb-a86d-49af-a6c7-ddd843f1fa81",
   "metadata": {},
   "source": [
    "### Residual Matrices\n",
    "\n",
    "It would be really nice if we could just stop here, and use subtraction matrices on their own, in order to extract the additional information we get as more attributes are added to a query, but it's not quite so simple. We actually need another concept to make it all work: Residual Matrices.\n",
    "\n",
    "For every $Q$ query we have, we will end up multiplying our dataset $x$ by a corresponding residual matrix $R_Q$, adding noise, giving us $R_Qx + N$ where $N$ is our noise.\n",
    "Then once we have all of these noisy answers, we can recombine them, expecting that each noisy answer is independant of one another, ($R_\\emptyset$ encodes the noisy total count, and $R_{\\{Attr\\}}$ encodes the noisy relative values between all the possible values of $Attr$ but not the total count).\n",
    "Thus if all of them are independant, we can recontruct answers to our original queries, without any contradictions.\n",
    "\n",
    "(A small nuance here is that we actually need to compute $R_Qx + N$ for the closure of all subqueries, e.g. if we have queries $\\emptyset$ and $\\{Attr_1,Attr_2\\}$ then we still need to compute residuals for $\\{Attr_1\\}$ and $\\{Attr_2\\}$ to allow us to reconstruct the answer to $\\{Attr_1,Attr_2\\}$).\n",
    "\n",
    "They're defined for some query $Q$ on an attribute domain $D$:\n",
    "$$\n",
    "R_{Q} = \\bigotimes_{attr \\in D} \\begin{cases}S_{|attr|} & attr \\in Q\\\\\\textbf{1}^{|attr|} & attr \\notin Q\\end{cases}\n",
    "$$\n",
    "\n",
    "I think this formulation is pretty hard to wrap your head around, so I want to spend a bit of time going through some examples to help give an intuition for how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65f4263-f6f8-440d-b74f-ddc58baaba9a",
   "metadata": {},
   "source": [
    "Consider the case of the empty \"count\" query:\n",
    "$$\n",
    "R_{\\emptyset} = \\bigotimes_{attr \\in D} 1^{|attr|} = 1^{\\prod_{attr \\in D} |attr|} = 1^{|x|}\n",
    "$$\n",
    "\n",
    "Because $attr \\notin \\emptyset$ will always be true. In this case, our residual matrix will always return a single number count of rows in $R_\\emptyset x = 1^{|x|}x = \\sum_{x_i \\in x} x_i$.\n",
    "This is basically what we expect, because there are no proper subsets of $\\emptyset$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35646e0-18c3-42d2-975a-fd1580b5b11d",
   "metadata": {},
   "source": [
    "Trying a more complicated set, we will actually use the code-base provided for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "896c3696-74c9-4d7f-bb73-c83d52a3fbb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., -1.,  0.,  0.,  0.],\n",
       "       [ 0.,  1., -1.,  0.,  0.],\n",
       "       [ 0.,  0.,  1., -1.,  0.],\n",
       "       [ 0.,  0.,  0.,  1., -1.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from class_resplan import *\n",
    "\n",
    "# Interestingly their codebase actually generates subtraction matrices of a different form,\n",
    "# with a diagonal of all 1's and the next diagonal up all -1's.\n",
    "#\n",
    "# From what we talk about above, this should still work, it removes all absolute information only leaving relative parts.\n",
    "# But it is notably different from the layout described in the actual paper, which focuses on the first column.\n",
    "# (I can't find any changes to the effect of this though)\n",
    "\n",
    "# Feel free to change this and see how it generates larger matrices\n",
    "i = 5\n",
    "subtract_matrix(i, False) # The \"False\" is to avoid generating a sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cec04cdd-48f1-4b62-a7ea-55f38ebf0860",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1., -1., -1., -1., -1., -1., -1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's pregenerate some subtraction matrices\n",
    "S = [subtract_matrix(i,False) for i in range(1,6)]\n",
    "\n",
    "# Let's play around with a simple domain\n",
    "# We have 3 attributes, (\"isAlive\" can either be True or False, \"hasDisease\" can be Yes or No, and \"numberOfEars\" can be 0, 1, or 2.\n",
    "attributes = {\"isAlive\" : [True,False], \"hasDisease\" : [\"Yes\",\"No\"], \"numberOfEars\" : [0,1,2]}\n",
    "\n",
    "# The code-base never uses an actual kronecker product because if you know you be doing something like:\n",
    "# kron(A,B) * vector, theres a faster way to do it using properties of the kronecker product.\n",
    "# So we'll need to use np.kron if we want to visualize stuff\n",
    "\n",
    "def residual_matrix(query, attrs):\n",
    "    res_mat = None\n",
    "    for name,domain in attrs.items():\n",
    "        cur_mat = None\n",
    "        if name in query:\n",
    "            cur_mat = subtract_matrix(len(domain),False)\n",
    "        else:\n",
    "            cur_mat = [1] * len(domain)\n",
    "        if res_mat is None:\n",
    "            res_mat = cur_mat\n",
    "        else:\n",
    "            res_mat = np.kron(res_mat,cur_mat)\n",
    "    return res_mat\n",
    "            \n",
    "query = set([\"isAlive\",\"hasDisease\"])\n",
    "residual_matrix(query, attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6fc91617-de2b-4c85-90fc-f7f1c90b9d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This allows us to name numbers so we can see what the results of multiplying by the matrix is\n",
    "# It looks really complicated because there's some additional rules to help with the printing (avoiding double negatives, stuff like that)\n",
    "# But really it just helps track what happens to a value over time, so we can see what field is semantically after mutliplying matrices\n",
    "class TracedNumber:\n",
    "    def __init__(self, n, name):\n",
    "        self.n = n\n",
    "        self.name = name\n",
    "    def __add__(self, other):\n",
    "        if isinstance(other, TracedNumber):\n",
    "            if other.name[0] == '-':\n",
    "                return TracedNumber(self.n + other.n, \"(\" + self.name + \")-(\" + other.name[1:] + \")\")\n",
    "            else:\n",
    "                return TracedNumber(self.n + other.n, \"(\" + self.name + \")+(\" + other.name + \")\")\n",
    "        else:\n",
    "            if other == 0:\n",
    "                return TracedNumer(self.n, self.name)\n",
    "            else:\n",
    "                if other > 0:\n",
    "                    return TracedNumber(self.n + other, \"(\" + self.name + \")+(\" + str(other) + \")\")\n",
    "                else:\n",
    "                    return TracedNumber(self.n + other, \"(\" + self.name + \")-(\" + str(-other) + \")\")\n",
    "    __radd__ = __add__\n",
    "    def __sub__(self, other):\n",
    "        return self + (other * (-1.0))\n",
    "    __rsub__ = __sub__\n",
    "    def __mul__(self,other):\n",
    "        if isinstance(other, TracedNumber):\n",
    "            return TracedNumer(self.n * other.n, \"(\" + self.name + \")*(\" + other.name + \")\")\n",
    "        else:\n",
    "            if other == 1:\n",
    "                return TracedNumber(self.n, self.name)\n",
    "            elif other == -1:\n",
    "                if self.name[0] == '-':\n",
    "                    return TracedNumber(-self.n, self.name[1:])\n",
    "                else:\n",
    "                    return TracedNumber(-self.n, \"-\"+self.name)\n",
    "            else:\n",
    "                return TracedNumber(self.n * other, \"(\" + self.name + \")*(\" + str(other) + \")\")\n",
    "    __rmul__ = __mul__\n",
    "    def __str__(self):\n",
    "        return \"[\" + str(self.n) + \",\" + self.name + \"]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ea1f7357-aa80-400c-a360-f9b07d038eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"[0,(True, 'Yes', 0)]\", \"[0,(True, 'Yes', 1)]\",\n",
       "       \"[1,(True, 'Yes', 2)]\", \"[1,(True, 'No', 0)]\",\n",
       "       \"[0,(True, 'No', 1)]\", \"[2,(True, 'No', 2)]\",\n",
       "       \"[0,(False, 'Yes', 0)]\", \"[1,(False, 'Yes', 1)]\",\n",
       "       \"[2,(False, 'Yes', 2)]\", \"[0,(False, 'No', 0)]\",\n",
       "       \"[1,(False, 'No', 1)]\", \"[1,(False, 'No', 2)]\"], dtype='<U21')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's a data set to play around with\n",
    "rows = [\n",
    "    {\"isAlive\":False,\"hasDisease\":\"No\",\"numberOfEars\":2},\n",
    "    {\"isAlive\":False,\"hasDisease\":\"Yes\",\"numberOfEars\":2},\n",
    "    {\"isAlive\":False,\"hasDisease\":\"Yes\",\"numberOfEars\":2},\n",
    "    {\"isAlive\":False,\"hasDisease\":\"No\",\"numberOfEars\":1},\n",
    "    {\"isAlive\":True,\"hasDisease\":\"No\",\"numberOfEars\":2},\n",
    "    {\"isAlive\":True,\"hasDisease\":\"Yes\",\"numberOfEars\":2},\n",
    "    {\"isAlive\":True,\"hasDisease\":\"No\",\"numberOfEars\":0},\n",
    "    {\"isAlive\":False,\"hasDisease\":\"Yes\",\"numberOfEars\":1},\n",
    "    {\"isAlive\":True,\"hasDisease\":\"No\",\"numberOfEars\":2},\n",
    "]\n",
    "\n",
    "# Encode it as a sum of one-hot vectors\n",
    "def encode_dataset(rows, attributes):\n",
    "    import functools\n",
    "    encoded_dataset = np.array([0] * functools.reduce(lambda x,y: x*y, [len(domain) for domain in attributes.values()]))\n",
    "    for row in rows:\n",
    "        encoded_row = None\n",
    "        for name,value in row.items():\n",
    "            domain = attributes[name]\n",
    "            index = domain.index(value)\n",
    "            encoded_attr = [0] * len(domain)\n",
    "            encoded_attr[index] = 1\n",
    "            if encoded_row is None:\n",
    "                encoded_row = encoded_attr\n",
    "            else:\n",
    "                encoded_row = np.kron(encoded_row,encoded_attr)\n",
    "        encoded_dataset += encoded_row\n",
    "    return encoded_dataset\n",
    "        \n",
    "encoded_dataset = encode_dataset(rows, attributes)\n",
    "\n",
    "# Trace what each number in the encoded data base actually represents, so we can see some semantic meaning in the output\n",
    "def trace_encoded_dataset(encoded_dataset, attributes):\n",
    "    encoded_dataset=encoded_dataset.tolist()\n",
    "    domains = [domain for domain in attributes.values()]\n",
    "    index = 0\n",
    "    for choice in itertools.product(*domains):\n",
    "        encoded_dataset[index] = TracedNumber(encoded_dataset[index],str(choice))\n",
    "        index += 1\n",
    "    return np.array(encoded_dataset)\n",
    "encoded_dataset = trace_encoded_dataset(encoded_dataset, attributes)\n",
    "\n",
    "# Useful for printing np.array's of TraceNumbers\n",
    "str_array = np.vectorize(str)\n",
    "str_array(encoded_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2cff5762-81f0-42df-87d9-0edb4f89fb1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residual Matrices:\n",
      "\t []  :  ['1' '1' '1' '1']\n",
      "\t ['isAlive']  :  [['1.0' '1.0' '-1.0' '-1.0']]\n",
      "\t ['hasDisease']  :  [['1.0' '-1.0' '1.0' '-1.0']]\n",
      "\t ['isAlive', 'hasDisease']  :  [['1.0' '-1.0' '-1.0' '1.0']]\n",
      "Residuals:\n",
      "\t []  :  [3,((((True, True))+((True, False)))+((False, True)))+((False, False))]\n",
      "\t ['isAlive']  :  ['[1,((((True, True))+((True, False)))-((False, True)))-((False, False))]']\n",
      "\t ['hasDisease']  :  ['[1,((((True, True))-((True, False)))+((False, True)))-((False, False))]']\n",
      "\t ['isAlive', 'hasDisease']  :  ['[-1,((((True, True))-((True, False)))-((False, True)))+((False, False))]']\n"
     ]
    }
   ],
   "source": [
    "# Let's try a simpler database\n",
    "attributes = {\"isAlive\":[True,False],\"hasDisease\":[True,False]}\n",
    "rows = [\n",
    "    {\"isAlive\":True,\"hasDisease\":True},\n",
    "    {\"isAlive\":True,\"hasDisease\":False},\n",
    "    {\"isAlive\":False,\"hasDisease\":True},\n",
    "]\n",
    "x = encode_dataset(rows,attributes)\n",
    "\n",
    "# Add tracing so we can see what happens when we play around with the numbers\n",
    "x = trace_encoded_dataset(x, attributes)\n",
    "\n",
    "# The List of Queries\n",
    "Q = [\n",
    "    [],\n",
    "    [\"isAlive\"],\n",
    "    [\"hasDisease\"],\n",
    "    [\"isAlive\", \"hasDisease\"],\n",
    "]\n",
    "\n",
    "residual_matrices = {}\n",
    "residuals = {}\n",
    "for query in Q:\n",
    "    residual_matrices[str(query)] = residual_matrix(query, attributes)\n",
    "    residuals[str(query)] = residual_matrices[str(query)] @ x\n",
    "\n",
    "print(\"Residual Matrices:\")\n",
    "for query,matrix in residual_matrices.items():\n",
    "    print(\"\\t\", query, \" : \", str_array(matrix))\n",
    "\n",
    "print(\"Residuals:\")\n",
    "for query,residual in residuals.items():\n",
    "    print(\"\\t\", query, \" : \", str_array(residual))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a579f-ab62-44a3-810a-22893007a79f",
   "metadata": {},
   "source": [
    "## Reconstruction\n",
    "\n",
    "From our marginals, (noise added or not), we then need to be able to reconstruct answers to the original queries which were submitted. Because of a very useful property of Kronecker products this isn't too bad though, because for both inverses and pseudo inverses,\n",
    "$$\n",
    "(A \\otimes B)^{-1} = A^{-1} \\otimes B^{-1}\n",
    "$$\n",
    "\n",
    "For this paper we actually only really care about \"pseudo inverses\" $A^+$, which have the properties\n",
    "$$\n",
    "\\begin{align*}\n",
    "(AA^+)A &= A\\\\\n",
    "A^+(AA^+) &= A^+\\\\\n",
    "(A^+A)^* &= A^+A\\\\\n",
    "(AA^+)^* &= AA^+\\\\\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c6785-5eb2-4250-858f-1427170039b0",
   "metadata": {},
   "source": [
    "We can calculate the pseudo-inverse of a subtraction matrix fairly easily, and so we can define a matrix $U$ such that\n",
    "\n",
    "$$\n",
    "U_Q = \\bigotimes_{attr \\in D} \\begin{cases}S_{|attr|}^{-1} & attr \\in Q\\\\ \\frac{1}{|attr|}\\textbf{1}^{|attr|} & attr \\notin Q\\\\ \\end{cases}\n",
    "$$\n",
    "\n",
    "And because the kronecker product of inverses is the inverse of the kronecker product, then $U_Q$ can be used as our pseudo-inverse to $R_Q$.\n",
    "So we can compute the residuals $R_q x$ for all subqueries $q \\in Q$, and add noise to them.\n",
    "Then if we compute $U_q$, we can \"invert\" by computing $U_q(R_q x + N)$. The sum of all these \"inverted\" values will then be our final noisy answer to the original query $Q$.\n",
    "\n",
    "Because if we have multiple queries $Q_1$, $Q_2$, ..., $Q_n$, then we just keep a set of sets for all of them. Then if there is any sub-query $q$ such that $q \\in Q_1$ and $q \\in Q_2$, we only compute it's residual once, and invert it once, and save the value. Then we can use it in reconstructing $Q_1$ and $Q_2$ (or as many more $Q$ as we have), which saves us work, and means that our answers will be consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29226df2-8f5e-4d06-a4cf-a05279ba5518",
   "metadata": {},
   "source": [
    "## Noise\n",
    "\n",
    "So far, we've pretty much ignored the problem of adding noise. I've come to the conclusion that it's somewhat orthogonal to the general issue of the matrix mechanism (consistency, reducing redundant computation) but the authors spend a decently large amount of time on it, so I didn't want to completely skip over it. It is also important if someone in the future wants to get ResidualPlanner to use the infinity-norm, because it's during the noise adding process that it comes up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b1c8fa-a6fc-4662-ad83-04e995962823",
   "metadata": {},
   "source": [
    "Adding the noise becomes an optimization problem, with a trade-off between the amount of privacy added, and the amount of accuracy lost from the noise.\n",
    "We can view the problem from two different perspectives, depending on whether we select an accuracy level and then try optimize for as much privacy as possible, or if we set a minimum privacy level, and then try to reduce the innaccuracy as much as possible.\n",
    "\n",
    "Generally we will be setting a \"privacy budget\" and then optimizing to be as accurate as possible while staying within that budget. Accuracy is modelled through the variance $\\sigma^2$ that applying some noise to each residual value will have on the answers which we compute to the final answer. Actually optimizing $\\sigma^2$ can be done with a linear programming solver, but the exact formulation depends on the cost function we select. Either minimizing the maximum variance, or minimizing the l2-norm of all variances. To be honest, I'm slightly conflicted after understand the paper more about the difference between optimizing for the \"maximum-variance\" and optimizing the infinity-norm of the variances. I'm guessing there's a subtle difference I'm not picking up on, and I know that the infinity-norm isn't actually just the maximum, but practically speaking it seems like you would expect the same results to me."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52487beb-a368-4d37-beb7-a8cd7053318c",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "I just wanted to say here at the end, thank you professor. I've had a great time in this class, and I think I've gotten a pretty solid foundations on the math behind differential privacy, and math in general. I hope this report is actually of some help, but I'm not 100% sure that it will be much easier to understand than the paper itself. After trying to explain these concepts myself (even after I got a solid grasp on them) I have a much higher degree of respect for how hard it is to communicate these topics (to any future students, if you want to read the paper itself, read Appendix B before anything else, it will be much much easier)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
